# Fact-Retrieval-and-Ensemble-Models-on-OpenBookQA

We investigated the OpenBookQA benchmark, a dataset of 5,957 elementary-level science multiple-choice questions designed to test understanding and application of 1,326 core science facts. This task evaluates fact-based reasoning, a critical area where language models often struggle. We benchmarked pre-trained and fine-tuned models, finding that they performed poorly, indicating a need for specialized approaches.

To address this, we implemented and tested two strategies: fact-retrieval and ensemble models. Fact-retrieval proved effective, boosting accuracy by 2â€“4%, while ensemble models lowered accuracy by around 2%. Our contributions include baseline evaluations for a range of mid-sized and large models, insights into the success of fact-retrieval methods, and proposals for further improvements. Additionally, we provide a survey of top-performing methods on OpenBookQA.
